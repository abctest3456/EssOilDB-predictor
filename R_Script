##info file for all plants into EssOilDB

plant_info <- read_csv("C:/Users/hadoop_pc/Desktop/info_files/Plant_info_9April.csv")

##info file for all compounds into EssOilDB

comp_info <- read.csv("C:/Users/hadoop_pc/Desktop/info_files/comp_info_9April.csv")

## Command to check dimension.
dim(plant_info)

## Command to check dimension.
dim(comp_info)


library(sqldf)

#### Getting article details for Lantana camara as Paper_code as unique field.
plant_info_LC <- sqldf("Select * from plant_info where Plant_name='Lantana camara'")


#### Getting all discussed compounds of Lantana camara using join operation and sqlquery.
comp_info_LC <- sqldf("Select * from plant_info_LC JOIN comp_info USING(Paper_code)")


## Example/Sample
## inner join
df3 <- sqldf("SELECT CustomerId, Product, State 
              FROM df1
              JOIN df2 USING(CustomerID)")

## left join (substitute 'right' for right join)
df4 <- sqldf("SELECT CustomerId, Product, State 
              FROM df1
              LEFT JOIN df2 USING(CustomerID)")
              
              
 ### getting count of unique values of each column 
 
 rapply(df3,function(x)length(unique(x)))
 
 unique(df3[c("Paper_code")])
 
 ### Get count for the unique values
 
 dim(unique(df3[c("Paper_code")]))
 
 ### merge the data frame 
 
  df4 <- merge(df1,df2, by = "Paper_code")
  
  ### Aggregate rows based on column values
  
  t <- aggregate(table[,2:4], list(table[,1]), function(x) paste0(unique(x)))
  
  dim(t)
  
  ### Get dataframe structure
  
  dim(df3)
  
  str(df3)
  
  ###Subset of the dataframe 
  data <- subset(comp_info_LC, select=c(2,11,13,14,17,18,19,20))
  
  #### Reorder by column number
data[c(1,3,2)]

  ### Reorder by column name
data[c("size", "id", "weight")]
  
  ### writing to csv file
   write.csv(data,"C:/Users/hadoop_pc/Desktop/info_files/comp_info_LC_RStudio.csv")
   
#######14 April

########18 April
sql query to get specified plant name columns corresponding compound_name and quantity.

> OG<- sqldf("Select * from one_sheet where Plant_name='Ocimum gratissimum'")
> OG <- subset(OG, select=c(2,15,4))
> OG<-t(OG)
> colnames(OG) <- OG[1,]
> OG<-OG[-1,]
> dim(OG)
[1]   2 305

### bind_rows over all above geberated data - OS,OB,LC,OG,NT.

> data_OS_OB_LC_OG_NT<-bind_rows(data.frame(OS),data.frame(OB),data.frame(LC),data.frame(OG),data.frame(NT))

#### removing duplicates (column names) from the data frame

data_OS_OB_LC_OG_NT<-data_OS_OB_LC_OG_NT[,!duplicated(colnames(data_OS_OB_LC_OG_NT))]

#### subsetting the dataframe

data_A[data_A$id %in% data_B$id,]

# Rename a column in R
colnames(data)[colnames(data)=="old_name"] <- "new_name"
> colnames(l)[colnames(l)=="X.Lantana.camara."] <- "plant_name

#### matrix forming

m1 <- matrix(C<-(0),nrow=1841, ncol=7159)

###load csv and convert to matrix

temp = read.csv("C:/Users/hadoop_pc/Desktop/empty_matrix_sheet.csv", sep=",", row.names=1)
temp1 <- as.matrix(temp) 
class(temp1)

### correcting column header

#### correcting column names as of same as column entries.
temp20 = read.csv("C:/Users/hadoop_pc/Desktop/empty_matrix_sheet.csv", sep=",", row.names=1,check.names=FALSE)

### code snippet

> temp200 = read.csv("C:/Users/hadoop_pc/Desktop/empty_matrix_sheet.csv", sep=",", row.names=1,check.names=FALSE)
> temp201 <- as.matrix(temp201)
Error in as.matrix(temp201) : object 'temp201' not found
> temp201 <- as.matrix(temp200)
> class(temp201)
[1] "matrix"
> dim(temp201)
[1] 1842 7159
> one_sheet_subset<-one_sheet_subset(2,1,3)
Error in one_sheet_subset(2, 1, 3) : 
  could not find function "one_sheet_subset"
> one_sheet_subset<-one_sheet_subset(,c(2,1,3))
Error in one_sheet_subset(, c(2, 1, 3)) : 
  could not find function "one_sheet_subset"
> one_sheet_subset <- one_sheet_subset[,c(2,1,3)]
> View(one_sheet_subset[1:5,1:3])

####lapply

a <- c("plant1","plant2","plant3","plant4","plant5");
b <- c("cmp1","cmp2","cmp3","cmp1","cmp2");
c <-rnorm(5);
mat <- cbind(a,b,c);
em_mat <- matrix(NA,5,3);
row.names(em_mat) <- a;
colnames(em_mat) <- unique(b);
b1 <- unique(b);

value<-c();
lapply(1:5, function(n1){
  lapply(1:3, function(n2){
    rm(value)
    sumat <- matrix(,,3);
    ss <- mat[mat[,1] %in% a[n1],];
    submat <- rbind(sumat, ss);
    value <- submat[submat[,2] %in% b1[n2],3];
    if(length(value) < 1){
      em_mat[n1,n2] <<- 0;
    }
    else{
      em_mat[n1,n2] <<- value;
    }
    
  })
})

### lappy over the dataset

lapply(1:1840, function(n1){
   lapply(1:7159, function(n2){
     rm(value)
    sumat <- matrix(,,3);
     ss <- one_sheet_subset[one_sheet_subset[,1] %in% plant_name_unique_vect[n1],];
     submat <- rbind(sumat, ss);
     submat <- rbind(sumat, ss);
     value <- submat[submat[,2] %in% comp_name_unique_vect[n2],3];
     if(length(value) < 1){
       em_mat_temp200[n1,n2] <<- 0;
        }
     else{
       em_mat_temp200[n1,n2] <<- value[1];
       }
     })
   })

#### raed csv file and convert to matrix data structure. ### em_mat_temp200_1_part is to be filled using run.R
em_mat_temp200_1_part <- read.csv("C:/Users/hadoop_pc/Desktop/em_mat_temp200_1_part.csv", sep=",", row.names=1,check.names=FALSE)

#### run.R
value<-c();
lapply(1:1840, function(n1){
   lapply(1:7159, function(n2){
     rm(value)
    sumat <- matrix(,,3);
     ss <- one_sheet_subset[one_sheet_subset[,1] %in% plant_name_unique_vect[n1],];
     submat <- rbind(sumat, ss);
     
     value <- submat[submat[,2] %in% comp_name_unique_vect[n2],3];
     if(length(value) < 1){
       em_mat_temp200_1_part[n1,n2] <<- 0;
        }
     else{
       em_mat_temp200_1_part[n1,n2] <<- value[1];
       }
     })
   })
   
   ### write data frames or matrixes to csv formatted file.
   write.csv(em_mat_temp200_1_part, "C:/Users/hadoop_pc/Desktop/em_mat_temp200_1_part.csv", sep = " ",row.names = TRUE, col.names = TRUE)
   
   #### KMEANS clustering 
   > dim(temp200cluster_dummy)
   > temp200cluster_dummy_kmeans <- kmeans(temp200cluster_dummy[,2:303], 20,nstart = 20)
   > temp200cluster_dummy_kmeans_table<-table(temp200cluster_dummy$plant_name,temp200cluster_dummy_kmeans$cluster)
> write.csv(temp200cluster_dummy_kmeans_table,"C:/Users/hadoop_pc/Desktop/temp200cluster_dummy_kmeans_table.csv")

### error linear model prediction
> ocimum_pr<-predict(ocimum_lm,em_mat_temp24A_filled_testset[,3:7161])
Error: variables ‘nerol’, ‘2-tridecanone’, ‘beta-damascone’ were specified with different types from the fit

###
  
   
### removing NA values
data[is.na(data)] <- 0

#### Hierarichal clustering and visualisation

> avg_dend_obj <- as.dendrogram(hc3)
> avg_col_dend <- color_branches(avg_dend_obj, h = 3)
> plot(avg_col_dend)
> hc1 <- hclust(d, method = "complete" )
> plot(hc1)

### scaling of the data (optional)
seeds_df_sc <- as.data.frame(scale(seeds_df))
summary(seeds_df_sc)

### generating distance matrix
dist_mat <- dist(seeds_df_sc, method = 'euclidean')

#### clustering the data - distance matrix
hclust_avg <- hclust(dist_mat, method = 'average')

### plot
plot(hclust_avg)
### cut the clustered data
cut_avg <- cutree(hclust_avg, k = 3)

### plot
plot(hclust_avg)
rect.hclust(hclust_avg , k = 3, border = 2:6)
abline(h = 3, col = 'red')

### tree visualisation

suppressPackageStartupMessages(library(dendextend))
avg_dend_obj <- as.dendrogram(hclust_avg)
avg_col_dend <- color_branches(avg_dend_obj, h = 3)
plot(avg_col_dend)

### Alternate way to plot and visualise the data
'''''''''''
# Dissimilarity matrix
d <- dist(df, method = "euclidean")
 
# Hierarchical clustering using Complete Linkage
hc1 <- hclust(d, method = "complete" )
 
# Plot the obtained dendrogram
plot(hc1)

''''''''''''
